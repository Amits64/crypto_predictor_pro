import os
import torch
import joblib
import numpy as np
import pandas as pd
from models.ensemble import EnsemblePredictor
from data.fetcher import fetch_enhanced_data
from preprocessing.anomalies import detect_anomalies
from preprocessing.indicators import calculate_heikin_ashi

INTERVAL_WEIGHTS = {
    "1m": 0.2,
    "5m": 0.5,
    "15m": 1.0,
    "1h": 3.0,
    "4h": 5.0,
    "1d": 10.0
}

def predict_for_interval(symbol, interval, seq_len, base_model_path):
    try:
        df = fetch_enhanced_data(symbol, interval, limit=seq_len + 50)
        if df is None or df.empty:
            return None, None, 0

        df = detect_anomalies(df)
        ha_df = calculate_heikin_ashi(df)
        df = df.join(ha_df[["ha_close"]])

        required_cols = [
            "close", "volume", "SMA_10", "SMA_20", "EMA_12", "EMA_26",
            "RSI", "MACD", "ATR", "ADX", "return_1", "volatility",
            "volume_ratio", "anomaly", "ha_close"
        ]
        if not all(col in df.columns for col in required_cols):
            return None, None, 0

        # Prepare input
        X = df.tail(seq_len)[required_cols].astype(float)
        X_df = pd.DataFrame(X, columns=required_cols)  # Preserve column names
        scaler_path = f"{base_model_path}/{symbol}_scaler_{interval}.pkl"
        model_path = f"{base_model_path}/{symbol}_model_{interval}.pt"

        if not os.path.exists(model_path) or not os.path.exists(scaler_path):
            return None, None, 0

        # Load model & scaler
        scaler = joblib.load(scaler_path)
        X_scaled = scaler.transform(X_df)  # Suppresses feature name warning
        X_tensor = torch.tensor(X_scaled.reshape(1, seq_len, -1), dtype=torch.float32)

        model = EnsemblePredictor(input_dim=X_tensor.shape[2], ensemble_size=5)
        model.load_state_dict(torch.load(model_path))
        model.eval()

        with torch.no_grad():
            pred, _, P, _ = model(X_tensor)
            conf = model.get_confidence(P)

        return pred.item(), conf, df["volume"].iloc[-1]

    except Exception as e:
        print(f"❌ Prediction error for {symbol} ({interval}): {e}")
        return None, None, 0


def fetch_multi_timeframe_data(symbol, intervals, history_size):
    dfs = {}
    for interval in intervals:
        try:
            # Fetch more data to allow asof merge tolerance
            df = fetch_enhanced_data(symbol, interval, history_size * 3)
            if df.empty:
                print(f"⚠️ No data for {interval}")
                continue

            # Preserve datetime for merge
            columns = []
            for col in df.columns:
                if col == "datetime":
                    columns.append(col)
                else:
                    columns.append(f"{col}_{interval}")
            df.columns = columns

            # Ensure datetime is sorted
            df = df.sort_values("datetime").reset_index(drop=True)
            dfs[interval] = df
        except Exception as e:
            print(f"❌ Error fetching data for {interval}: {e}")
    return dfs
